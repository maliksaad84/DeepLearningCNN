{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torch\\cuda\\__init__.py:82: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE ONLY CPU!\n",
      "=> loading checkpoint\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss: (0.1755) | Acc: (92.50%) (37/40)\n",
      "Epoch: 1 | Batch_idx: 10 |  Loss: (0.1487) | Acc: (94.77%) (417/440)\n",
      "Epoch: 1 | Batch_idx: 20 |  Loss: (0.1589) | Acc: (93.93%) (789/840)\n",
      "Epoch: 1 | Batch_idx: 30 |  Loss: (0.1510) | Acc: (94.60%) (1173/1240)\n",
      "Epoch: 1 | Batch_idx: 40 |  Loss: (0.1347) | Acc: (95.43%) (1565/1640)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss: (0.1285) | Acc: (95.64%) (1951/2040)\n",
      "Epoch: 1 | Batch_idx: 60 |  Loss: (0.1256) | Acc: (95.74%) (2336/2440)\n",
      "Epoch: 1 | Batch_idx: 70 |  Loss: (0.1217) | Acc: (95.88%) (2723/2840)\n",
      "Epoch: 1 | Batch_idx: 80 |  Loss: (0.1244) | Acc: (95.80%) (3104/3240)\n",
      "Epoch: 1 | Batch_idx: 90 |  Loss: (0.1203) | Acc: (95.96%) (3493/3640)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss: (0.1241) | Acc: (95.92%) (3875/4040)\n",
      "Epoch: 1 | Batch_idx: 110 |  Loss: (0.1265) | Acc: (95.79%) (4253/4440)\n",
      "Epoch: 1 | Batch_idx: 120 |  Loss: (0.1301) | Acc: (95.58%) (4626/4840)\n",
      "Epoch: 1 | Batch_idx: 130 |  Loss: (0.1313) | Acc: (95.55%) (5007/5240)\n",
      "Epoch: 1 | Batch_idx: 140 |  Loss: (0.1306) | Acc: (95.60%) (5392/5640)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss: (0.1285) | Acc: (95.71%) (5781/6040)\n",
      "Epoch: 1 | Batch_idx: 160 |  Loss: (0.1277) | Acc: (95.70%) (6163/6440)\n",
      "Epoch: 1 | Batch_idx: 170 |  Loss: (0.1285) | Acc: (95.70%) (6546/6840)\n",
      "Epoch: 1 | Batch_idx: 180 |  Loss: (0.1284) | Acc: (95.72%) (6930/7240)\n",
      "Epoch: 1 | Batch_idx: 190 |  Loss: (0.1296) | Acc: (95.68%) (7310/7640)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss: (0.1320) | Acc: (95.61%) (7687/8040)\n",
      "Epoch: 1 | Batch_idx: 210 |  Loss: (0.1322) | Acc: (95.63%) (8071/8440)\n",
      "Epoch: 1 | Batch_idx: 220 |  Loss: (0.1310) | Acc: (95.67%) (8457/8840)\n",
      "Epoch: 1 | Batch_idx: 230 |  Loss: (0.1317) | Acc: (95.66%) (8839/9240)\n",
      "Epoch: 1 | Batch_idx: 240 |  Loss: (0.1308) | Acc: (95.67%) (9223/9640)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss: (0.1317) | Acc: (95.65%) (9603/10040)\n",
      "Epoch: 1 | Batch_idx: 260 |  Loss: (0.1322) | Acc: (95.63%) (9984/10440)\n",
      "Epoch: 1 | Batch_idx: 270 |  Loss: (0.1330) | Acc: (95.61%) (10364/10840)\n",
      "Epoch: 1 | Batch_idx: 280 |  Loss: (0.1327) | Acc: (95.60%) (10746/11240)\n",
      "Epoch: 1 | Batch_idx: 290 |  Loss: (0.1329) | Acc: (95.61%) (11129/11640)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss: (0.1338) | Acc: (95.58%) (11508/12040)\n",
      "Epoch: 1 | Batch_idx: 310 |  Loss: (0.1326) | Acc: (95.64%) (11897/12440)\n",
      "Epoch: 1 | Batch_idx: 320 |  Loss: (0.1322) | Acc: (95.62%) (12278/12840)\n",
      "Epoch: 1 | Batch_idx: 330 |  Loss: (0.1326) | Acc: (95.63%) (12661/13240)\n",
      "Epoch: 1 | Batch_idx: 340 |  Loss: (0.1327) | Acc: (95.62%) (13043/13640)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss: (0.1320) | Acc: (95.63%) (13426/14040)\n",
      "Epoch: 1 | Batch_idx: 360 |  Loss: (0.1317) | Acc: (95.66%) (13813/14440)\n",
      "Epoch: 1 | Batch_idx: 370 |  Loss: (0.1302) | Acc: (95.71%) (14204/14840)\n",
      "Epoch: 1 | Batch_idx: 380 |  Loss: (0.1290) | Acc: (95.73%) (14590/15240)\n",
      "Epoch: 1 | Batch_idx: 390 |  Loss: (0.1301) | Acc: (95.68%) (14965/15640)\n",
      "Epoch: 1 | Batch_idx: 400 |  Loss: (0.1315) | Acc: (95.65%) (15343/16040)\n",
      "Epoch: 1 | Batch_idx: 410 |  Loss: (0.1311) | Acc: (95.65%) (15725/16440)\n",
      "Epoch: 1 | Batch_idx: 420 |  Loss: (0.1305) | Acc: (95.68%) (16113/16840)\n",
      "Epoch: 1 | Batch_idx: 430 |  Loss: (0.1310) | Acc: (95.66%) (16491/17240)\n",
      "Epoch: 1 | Batch_idx: 440 |  Loss: (0.1304) | Acc: (95.68%) (16878/17640)\n",
      "Epoch: 1 | Batch_idx: 450 |  Loss: (0.1303) | Acc: (95.68%) (17261/18040)\n",
      "Epoch: 1 | Batch_idx: 460 |  Loss: (0.1298) | Acc: (95.68%) (17644/18440)\n",
      "Epoch: 1 | Batch_idx: 470 |  Loss: (0.1310) | Acc: (95.64%) (18019/18840)\n",
      "Epoch: 1 | Batch_idx: 480 |  Loss: (0.1298) | Acc: (95.68%) (18408/19240)\n",
      "Epoch: 1 | Batch_idx: 490 |  Loss: (0.1302) | Acc: (95.67%) (18789/19640)\n",
      "Epoch: 1 | Batch_idx: 500 |  Loss: (0.1296) | Acc: (95.68%) (19174/20040)\n",
      "Epoch: 1 | Batch_idx: 510 |  Loss: (0.1300) | Acc: (95.67%) (19554/20440)\n",
      "Epoch: 1 | Batch_idx: 520 |  Loss: (0.1295) | Acc: (95.67%) (19938/20840)\n",
      "Epoch: 1 | Batch_idx: 530 |  Loss: (0.1294) | Acc: (95.67%) (20321/21240)\n",
      "Epoch: 1 | Batch_idx: 540 |  Loss: (0.1297) | Acc: (95.66%) (20700/21640)\n",
      "Epoch: 1 | Batch_idx: 550 |  Loss: (0.1296) | Acc: (95.67%) (21086/22040)\n",
      "Epoch: 1 | Batch_idx: 560 |  Loss: (0.1294) | Acc: (95.66%) (21467/22440)\n",
      "Epoch: 1 | Batch_idx: 570 |  Loss: (0.1294) | Acc: (95.67%) (21852/22840)\n",
      "Epoch: 1 | Batch_idx: 580 |  Loss: (0.1292) | Acc: (95.67%) (22234/23240)\n",
      "Epoch: 1 | Batch_idx: 590 |  Loss: (0.1286) | Acc: (95.69%) (22622/23640)\n",
      "Epoch: 1 | Batch_idx: 600 |  Loss: (0.1281) | Acc: (95.70%) (23007/24040)\n",
      "Epoch: 1 | Batch_idx: 610 |  Loss: (0.1284) | Acc: (95.70%) (23389/24440)\n",
      "Epoch: 1 | Batch_idx: 620 |  Loss: (0.1281) | Acc: (95.70%) (23773/24840)\n",
      "Epoch: 1 | Batch_idx: 630 |  Loss: (0.1283) | Acc: (95.69%) (24153/25240)\n",
      "Epoch: 1 | Batch_idx: 640 |  Loss: (0.1283) | Acc: (95.69%) (24536/25640)\n",
      "Epoch: 1 | Batch_idx: 650 |  Loss: (0.1283) | Acc: (95.70%) (24921/26040)\n",
      "Epoch: 1 | Batch_idx: 660 |  Loss: (0.1283) | Acc: (95.71%) (25305/26440)\n",
      "Epoch: 1 | Batch_idx: 670 |  Loss: (0.1278) | Acc: (95.73%) (25694/26840)\n",
      "Epoch: 1 | Batch_idx: 680 |  Loss: (0.1277) | Acc: (95.74%) (26079/27240)\n",
      "Epoch: 1 | Batch_idx: 690 |  Loss: (0.1276) | Acc: (95.73%) (26459/27640)\n",
      "Epoch: 1 | Batch_idx: 700 |  Loss: (0.1283) | Acc: (95.72%) (26840/28040)\n",
      "Epoch: 1 | Batch_idx: 710 |  Loss: (0.1283) | Acc: (95.72%) (27223/28440)\n",
      "Epoch: 1 | Batch_idx: 720 |  Loss: (0.1284) | Acc: (95.72%) (27605/28840)\n",
      "Epoch: 1 | Batch_idx: 730 |  Loss: (0.1285) | Acc: (95.72%) (27989/29240)\n",
      "Epoch: 1 | Batch_idx: 740 |  Loss: (0.1286) | Acc: (95.72%) (28371/29640)\n",
      "Epoch: 1 | Batch_idx: 750 |  Loss: (0.1284) | Acc: (95.71%) (28751/30040)\n",
      "Epoch: 1 | Batch_idx: 760 |  Loss: (0.1283) | Acc: (95.71%) (29133/30440)\n",
      "Epoch: 1 | Batch_idx: 770 |  Loss: (0.1281) | Acc: (95.70%) (29515/30840)\n",
      "Epoch: 1 | Batch_idx: 780 |  Loss: (0.1280) | Acc: (95.70%) (29897/31240)\n",
      "Epoch: 1 | Batch_idx: 790 |  Loss: (0.1281) | Acc: (95.70%) (30280/31640)\n",
      "Epoch: 1 | Batch_idx: 800 |  Loss: (0.1277) | Acc: (95.71%) (30667/32040)\n",
      "Epoch: 1 | Batch_idx: 810 |  Loss: (0.1285) | Acc: (95.69%) (31043/32440)\n",
      "Epoch: 1 | Batch_idx: 820 |  Loss: (0.1284) | Acc: (95.70%) (31427/32840)\n",
      "Epoch: 1 | Batch_idx: 830 |  Loss: (0.1280) | Acc: (95.72%) (31816/33240)\n",
      "Epoch: 1 | Batch_idx: 840 |  Loss: (0.1282) | Acc: (95.71%) (32197/33640)\n",
      "Epoch: 1 | Batch_idx: 850 |  Loss: (0.1280) | Acc: (95.72%) (32583/34040)\n",
      "Epoch: 1 | Batch_idx: 860 |  Loss: (0.1280) | Acc: (95.72%) (32965/34440)\n",
      "Epoch: 1 | Batch_idx: 870 |  Loss: (0.1278) | Acc: (95.72%) (33349/34840)\n",
      "Epoch: 1 | Batch_idx: 880 |  Loss: (0.1275) | Acc: (95.73%) (33737/35240)\n",
      "Epoch: 1 | Batch_idx: 890 |  Loss: (0.1269) | Acc: (95.75%) (34125/35640)\n",
      "Epoch: 1 | Batch_idx: 900 |  Loss: (0.1266) | Acc: (95.76%) (34512/36040)\n",
      "Epoch: 1 | Batch_idx: 910 |  Loss: (0.1268) | Acc: (95.75%) (34892/36440)\n",
      "Epoch: 1 | Batch_idx: 920 |  Loss: (0.1266) | Acc: (95.76%) (35279/36840)\n",
      "Epoch: 1 | Batch_idx: 930 |  Loss: (0.1263) | Acc: (95.77%) (35665/37240)\n",
      "Epoch: 1 | Batch_idx: 940 |  Loss: (0.1262) | Acc: (95.77%) (36049/37640)\n",
      "Epoch: 1 | Batch_idx: 950 |  Loss: (0.1267) | Acc: (95.75%) (36422/38040)\n",
      "Epoch: 1 | Batch_idx: 960 |  Loss: (0.1269) | Acc: (95.74%) (36801/38440)\n",
      "Epoch: 1 | Batch_idx: 970 |  Loss: (0.1269) | Acc: (95.74%) (37184/38840)\n",
      "Epoch: 1 | Batch_idx: 980 |  Loss: (0.1271) | Acc: (95.73%) (37566/39240)\n",
      "Epoch: 1 | Batch_idx: 990 |  Loss: (0.1273) | Acc: (95.73%) (37946/39640)\n",
      "Epoch: 1 | Batch_idx: 1000 |  Loss: (0.1272) | Acc: (95.73%) (38331/40040)\n",
      "Epoch: 1 | Batch_idx: 1010 |  Loss: (0.1270) | Acc: (95.73%) (38713/40440)\n",
      "Epoch: 1 | Batch_idx: 1020 |  Loss: (0.1269) | Acc: (95.72%) (39091/40840)\n",
      "Epoch: 1 | Batch_idx: 1030 |  Loss: (0.1268) | Acc: (95.73%) (39478/41240)\n",
      "Epoch: 1 | Batch_idx: 1040 |  Loss: (0.1264) | Acc: (95.75%) (39869/41640)\n",
      "Epoch: 1 | Batch_idx: 1050 |  Loss: (0.1265) | Acc: (95.75%) (40253/42040)\n",
      "Epoch: 1 | Batch_idx: 1060 |  Loss: (0.1263) | Acc: (95.76%) (40640/42440)\n",
      "Epoch: 1 | Batch_idx: 1070 |  Loss: (0.1269) | Acc: (95.75%) (41018/42840)\n",
      "Epoch: 1 | Batch_idx: 1080 |  Loss: (0.1267) | Acc: (95.75%) (41403/43240)\n",
      "Epoch: 1 | Batch_idx: 1090 |  Loss: (0.1265) | Acc: (95.74%) (41782/43640)\n",
      "Epoch: 1 | Batch_idx: 1100 |  Loss: (0.1262) | Acc: (95.75%) (42169/44040)\n",
      "Epoch: 1 | Batch_idx: 1110 |  Loss: (0.1263) | Acc: (95.74%) (42548/44440)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch_idx: 1120 |  Loss: (0.1261) | Acc: (95.74%) (42932/44840)\n",
      "Epoch: 1 | Batch_idx: 1130 |  Loss: (0.1262) | Acc: (95.74%) (43314/45240)\n",
      "Epoch: 1 | Batch_idx: 1140 |  Loss: (0.1259) | Acc: (95.75%) (43701/45640)\n",
      "Epoch: 1 | Batch_idx: 1150 |  Loss: (0.1258) | Acc: (95.75%) (44085/46040)\n",
      "Epoch: 1 | Batch_idx: 1160 |  Loss: (0.1256) | Acc: (95.77%) (44474/46440)\n",
      "Epoch: 1 | Batch_idx: 1170 |  Loss: (0.1256) | Acc: (95.76%) (44854/46840)\n",
      "Epoch: 1 | Batch_idx: 1180 |  Loss: (0.1255) | Acc: (95.77%) (45240/47240)\n",
      "Epoch: 1 | Batch_idx: 1190 |  Loss: (0.1256) | Acc: (95.77%) (45623/47640)\n",
      "Epoch: 1 | Batch_idx: 1200 |  Loss: (0.1254) | Acc: (95.77%) (46009/48040)\n",
      "Epoch: 1 | Batch_idx: 1210 |  Loss: (0.1254) | Acc: (95.77%) (46391/48440)\n",
      "Epoch: 1 | Batch_idx: 1220 |  Loss: (0.1250) | Acc: (95.79%) (46784/48840)\n",
      "Epoch: 1 | Batch_idx: 1230 |  Loss: (0.1249) | Acc: (95.80%) (47171/49240)\n",
      "Epoch: 1 | Batch_idx: 1240 |  Loss: (0.1245) | Acc: (95.81%) (47560/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1833) | Acc: (94.28%) (9428/10000)\n",
      "13 hours 17 mins 27 secs for training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 40\n",
    "learning_rate = 0.001\n",
    "root_dir = 'drive/app/cifar10/'\n",
    "default_directory = 'drive/app/torch/save_models'\n",
    "\n",
    "# Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),                               \n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "# automatically download\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def initialize_model( num_classes, use_pretrained=True):\n",
    "\n",
    "    model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "    input_size = 224\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model, input_size = initialize_model(10, use_pretrained=True)\n",
    "'''\n",
    "#loading Pretrainined VGG\n",
    "model= models.vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "\n",
    "'''\n",
    "model = model.to(device)\n",
    "#optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10) #Changed the optimizer to Adagrad \n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(start_epoch, 2):\n",
    "\n",
    "    if epoch < 20:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 40:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test()  \n",
    "\n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
